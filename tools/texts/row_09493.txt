article: Facebook has rolled out a new system to try and identify and help users having suicidal thoughts. It allows concerned friends  to report content for review. After reviewing a report and confirming that it may indicate suicidal feelings in a user, Facebook will send that user notifications stating that a friend has attempted to help and offering access to suicide prevention resources. The San Francisco social network's new system allows concerned friends to to report content for review, and Facebook can then offer the user more help. The tools build on a reporting feature implemented in 2011, which passed details to the National Suicide Prevention Lifeline in the US, and the Samaritans in the UK. 'Keeping you safe is our most important responsibility on Facebook,' said Rob Boyle of Facebook. 'Today, at our fifth Compassion Research Day, we announced updated tools that provide more resources, advice and support to people who may be struggling with suicidal thoughts and their concerned friends and family members.' The firm said it had worked with mental health organizations Forefront, Now Matters Now, the National Suicide Prevention Lifeline, Save.org and others on the updates, and consulted with people who had lived experience with self-injury or suicide. 'One of the first things these organizations discussed with us was how much connecting with people who care can help those in distress. 'If someone on Facebook sees a direct threat of suicide, we ask that they contact their local emergency services immediately. 'We also ask them to report any troubling content to us.' Facebook has set up teams working around the world, 24/7, who review any report that comes in. They prioritize the most serious reports, like self-injury, and send help and resources to those in distress. The firm said it had worked with mental health organizations Forefront, Now Matters Now, the National Suicide Prevention Lifeline, Save.org and others on the updates, and consulted with people who had lived experience with self-injury or suicide. For those who may need help we have significantly expanded the support and resources that are available to them the next time they log on to Facebook after we review a report of something they’ve posted. Besides encouraging them to connect with a mental health expert at the National Suicide Prevention Lifeline, we now also give them the option of reaching out to a friend, and provide tips and advice on how they can work through these feelings. All of these resources were created in conjunction with our clinical and academic partners. We’re also providing new resources and support to the person who flagged the troubling post, including options for them to call or message their distressed friend letting them know they care, or reaching out to another friend or a trained professional at a suicide hotline for support.
highlights: New system allows concerned friends to report content for review . Facebook will send user notifications stating that a friend is worried . Will offer access to suicide prevention resources . Initially offered in the US, but expected to be expanded to other countries .
id: 0ffba30235410ed99b6417a59c97fcf4b2b72748
chatgpt: Facebook is taking steps to address concerns about harmful or inappropriate content on their platform. The social media giant has launched a new system that allows concerned friends to report content for review. The system aims to identify and intervene in situations where someone may be at risk of harm or may be experiencing difficult emotions. The system works by allowing friends to flag content that they believe may be harmful. If a friend reports something, Facebook will send notifications to the user, stating that someone is worried about them. The notification will also offer access to suicide prevention resources. The new system was launched in the US, where it has received positive feedback from users. It is expected to be expanded to other countries in the near future. One of the biggest challenges facing social media platforms like Facebook is how to address concerns about the presence of harmful or inappropriate content. While the platform has tools in place that allow users to report content, these tools can be difficult to use and may not always be effective. The new system aims to address this problem by empowering friends to take an active role in reporting content. By giving users access to a simple and easy-to-use reporting system, Facebook hopes to make it easier for people to get the help they need. The suicide prevention resources offered through the new system can be a lifeline for those who are struggling or experiencing difficult emotions. The resources are designed to provide support and guidance, and can help people access professional help if needed. The launch of the new system is just the latest step in Facebook's ongoing efforts to address concerns about harmful content on their platform. The company has invested heavily in developing tools and resources to help users stay safe and protect themselves from harm. In recent years, Facebook has also introduced a range of other measures aimed at improving user safety. These include measures to combat hate speech and fake news, as well as tools to help users protect their personal information and privacy. While Facebook's efforts to address harmful content on their platform are commendable, there is still a long way to go. The company has faced criticism in the past for their handling of certain situations, and some users have expressed concerns about the effectiveness of their reporting tools. However, the launch of the new system shows that Facebook is committed to continuously improving their platform and making it a safer place for users. By giving users more control over their own safety and well-being, Facebook is taking an important step towards creating a safer and more supportive online community.